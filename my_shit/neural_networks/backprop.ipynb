{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a83d296",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc2314",
   "metadata": {},
   "source": [
    "### Gradients in a Two-Layer Neural Network\n",
    "\n",
    "For the network:\n",
    "$\n",
    "f(x; \\mathbf{w}) = \\text{softmax}(\\mathbf{W}_2 \\, \\text{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
    "$\n",
    "\n",
    "#### Gradient with respect to \\( $\\mathbf{b}_1$ \\):\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\text{CE}(y, f(x, w))}{\\partial \\mathbf{b}_1} = \n",
    "\\frac{\\partial \\text{CE}}{\\partial f} \\cdot\n",
    "\\frac{\\partial f}{\\partial h_2} \\cdot\n",
    "\\frac{\\partial h_2}{\\partial h_1} \\cdot\n",
    "\\frac{\\partial h_1}{\\partial \\mathbf{b}_1}\n",
    "$\n",
    "\n",
    "#### Gradient with respect to \\( $\\mathbf{W}_2$ \\):\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\text{CE}(y, f(x, w))}{\\partial \\mathbf{W}_2} = \n",
    "\\frac{\\partial \\text{CE}}{\\partial f} \\cdot\n",
    "\\frac{\\partial f}{\\partial h_2} \\cdot\n",
    "\\frac{\\partial h_2}{\\partial \\mathbf{W}_2}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5724b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂CE/∂z = [ 0.1 -0.4  0.3]\n",
      "∂ReLU/∂b1 = diag [0 1 1]\n",
      "Mean cross entropy = 0.45106831698704936\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 18b ---\n",
    "# Given values\n",
    "f = np.array([0.1, 0.6, 0.3])  # softmax output\n",
    "y = 1  # target class index (Python 0-based, so y=2 in question means index 1)\n",
    "\n",
    "# 1. Partial derivative of cross-entropy wrt logits (pre-softmax)\n",
    "dCE_dz = f.copy()\n",
    "dCE_dz[y] -= 1\n",
    "print(\"∂CE/∂z =\", dCE_dz)\n",
    "\n",
    "# 2. Derivative of ReLU wrt input\n",
    "z = np.array([-0.5, 0.3, 2])\n",
    "dReLU_db1 = np.diag((z > 0).astype(int))\n",
    "print(\"∂ReLU/∂b1 = diag\", np.diag(dReLU_db1))\n",
    "\n",
    "# --- 18c ---\n",
    "# Training samples: (logits, true label)\n",
    "samples = [\n",
    "    (np.array([0.1, 0.5]), 0),   # y=1\n",
    "    (np.array([-1, 1]), 1),      # y=2\n",
    "    (np.array([0, -1]), 0)       # y=1\n",
    "]\n",
    "\n",
    "def softmax(logits):\n",
    "    e = np.exp(logits - np.max(logits))\n",
    "    return e / e.sum()\n",
    "\n",
    "cross_entropies = []\n",
    "for logits, label in samples:\n",
    "    probs = softmax(logits)\n",
    "    ce = -np.log(probs[label])\n",
    "    cross_entropies.append(ce)\n",
    "\n",
    "mean_ce = np.mean(cross_entropies)\n",
    "print(\"Mean cross entropy =\", mean_ce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
